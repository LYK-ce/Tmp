#Presented by KeJi
#Date: 2025-12-20

# Mamba Selective Scan CPU 优化设计文档

## 目标

在**不使用 SIMD** 的前提下，通过**内存优化**和**算法改进**实现 3-5x 性能提升。

---

## 核心优化方向

### 1. 算子融合（消除冗余内存访问）

**当前问题** ([`model.py:318`](model.py:318)):
```python
x = deltaA[:, i] * x + deltaB_u[:, i]
```

**分解为**:
- `temp = deltaA[:, i] * x`      → 读2次，写1次
- `x = temp + deltaB_u[:, i]`    → 读2次，写1次
- **总计**: 读4次，写2次

**优化方案**: 单个融合操作
- `x = fused_muladd(deltaA[i], x, deltaB_u[i])`
- **总计**: 读3次，写1次
- **收益**: 减少 33% 内存访问

---

### 2. 内存布局优化（提高缓存命中率）

**当前问题**: 数据不连续
```python
deltaA[:, i]  # 跨 L 维度跳跃访问
# 布局: (B, L, D, N)
#        └─ 每次取一个 slice，stride 很大
```

**优化方案**: 转置为缓存友好布局
```cpp
// 预处理: 转置维度
// (B, L, D, N) → (B, D, L, N)
// 现在 L 维度连续，顺序访问
```

**收益**: 
- 提高 CPU 缓存命中率
- 减少 cache miss: **20-30%**

---

### 3. 预分配输出（避免动态扩容）

**当前问题** ([`model.py:320`](model.py:320)):
```python
ys = []
for i in range(128):
    ys.append(y)  # 动态 list，可能重新分配
```

**优化方案**: 预分配张量
```cpp
// C++: 预分配连续内存
float* output = new float[batch * seq_len * d_inner];

for (int t = 0; t < seq_len; t++) {
    // 直接写入: output[b][t][d] = y
}
```

**收益**:
- 消除 128 次 append 开销
- 内存连续，后续处理更快

---

### 4. 减少临时张量分配

**当前问题**: 每次迭代创建新张量
```python
deltaA[:, i]      # 创建临时视图
deltaB_u[:, i]    # 创建临时视图
C[:, i, :]        # 创建临时视图
```

**优化方案**: 使用指针直接访问
```cpp
// 直接指针操作，无临时对象
const float* deltaA_ptr = &deltaA_data[b * L * D * N + i * D * N];
const float* deltaB_u_ptr = &deltaB_u_data[b * L * D * N + i * D * N];

// 直接计算，无中间张量
for (int idx = 0; idx < D * N; idx++) {
    h[idx] = deltaA_ptr[idx] * h[idx] + deltaB_u_ptr[idx];
}
```

**收益**:
- 消除张量元数据创建
- 减少内存分配器调用

---

### 5. 循环重排（提高数据局部性）

**当前问题**: 嵌套循环顺序不优
```python
for t in range(L):           # 时间步
    for d in range(D):       # 通道
        for n in range(N):   # 状态维度
            # 访问 A[d][n]，跨 d 维度跳跃
```

**优化方案**: 调整循环顺序
```cpp
// 优化: 先遍历 d，再遍历 t
for (int d = 0; d < D; d++) {
    // A[d] 可以保持在缓存中
    for (int t = 0; t < L; t++) {
        for (int n = 0; n < N; n++) {
            // 连续访问 A[d][n]
        }
    }
}
```

**收益**:
- `A` 矩阵可常驻 L1 缓存
- 减少缓存换出

---

### 6. einsum 替换（高效矩阵乘法）

**当前问题** ([`model.py:319`](model.py:319)):
```python
y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')
```

**优化方案**: 手写点积循环
```cpp
// 针对小矩阵 (256×16) 特化
for (int d = 0; d < d_inner; d++) {
    float dot = 0.0f;
    const float* h_ptr = &h[d * n];  // 指向 h[d, :]
    
    // 展开的点积
    for (int i = 0; i < n; i++) {
        dot += h_ptr[i] * C[i];
    }
    
    output[d] = dot + D[d] * u[d];
}
```

**收益**:
- 消除 einsum 解析开销
- 针对小矩阵特化
- 更好的编译器优化潜力

---

## 优化效果预估

| 优化技术 | 预期提升 | 实现难度 | 优先级 |
|---------|---------|---------|--------|
| **算子融合** | 1.5-2x | 🟢 低 | P0 |
| **预分配输出** | 1.1-1.2x | 🟢 低 | P0 |
| **内存布局优化** | 1.2-1.3x | 🟡 中 | P1 |
| **替换 einsum** | 1.3-1.5x | 🟢 低 | P0 |
| **减少临时分配** | 1.1-1.15x | 🟡 中 | P1 |
| **循环重排** | 1.1-1.2x | 🟡 中 | P2 |

**组合效果**: 1.5 × 1.2 × 1.3 × 1.3 × 1.15 × 1.15 ≈ **4-5x**

---

## 实现策略

### 阶段 1: 基础优化（2-3天）
✅ P0 优先级
- 算子融合
- 预分配输出
- 替换 einsum

**目标**: 17ms → **6-8ms** (2-3x)

### 阶段 2: 内存优化（3-5天）
✅ P1 优先级
- 内存布局转置
- 减少临时分配

**目标**: 6-8ms → **4-5ms** (3-4x)

### 阶段 3: 细节优化（可选）
✅ P2 优先级
- 循环重排
- 编译器友好改写

**目标**: 4-5ms → **3-4ms** (4-5x)

---

## 技术栈

- **语言**: C++17
- **框架**: PyTorch C++ API (ATen)
- **绑定**: pybind11
- **编译**: -O3 优化
- **并行**: OpenMP (可选)

---

## 下一步讨论要点

1. 内存布局转置的最优策略
2. 融合算子的具体实现细节
3. 循环结构的重新设计
4. 数值稳定性保证
5. 单元测试和性能benchmark

---

**核心理念**: **减少内存访问 > 优化计算** (CPU 性能主要受限于内存带宽)

---

## 实验结果：两阶段计算优化

### 实验时间
2025-12-20

### 优化方案
基于对原始算法的深入分析，提出了一种**两阶段计算**的优化策略，在 PyTorch 层面实现显著性能提升。

#### 核心思想
将原本交错的隐藏状态计算和输出计算分离为两个独立阶段，复用 `deltaB_u` 的存储空间：

**阶段 1：一次性计算所有隐藏状态**
```python
# 预计算 deltaB_u (b, l, d_in, n)
deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')

# 递推计算隐藏状态，原地存储
for i in range(1, l):
    deltaB_u[:, i] = deltaA[:, i] * deltaB_u[:, i-1] + deltaB_u[:, i]
# 现在 deltaB_u[:, i] 存储的是每个时间步的隐藏状态 x[i]
```

**阶段 2：批量计算所有输出**
```python
# 一次性计算所有输出 y
y = einsum(deltaB_u, C, 'b l d_in n, b l n -> b l d_in')
y = y + u * D
```

#### 对比原始实现

**原始版本** ([`model.py:315-321`](model.py:315)):
```python
x = torch.zeros((b, d_in, n), device=deltaA.device)
ys = []
for i in range(l):  # l = 128
    x = deltaA[:, i] * x + deltaB_u[:, i]      # 逐个更新 x
    y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')  # 逐个计算 y
    ys.append(y)                                # 动态 append
y = torch.stack(ys, dim=1)                      # 最后 stack
```

**优化版本** ([`model_new.py:332-338`](model_new.py:332)):
```python
deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))
deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')

for i in range(1, l):
    deltaB_u[:, i] = deltaA[:, i] * deltaB_u[:, i-1] + deltaB_u[:, i]

y = einsum(deltaB_u, C, 'b l d_in n, b l n -> b l d_in')
y = y + u * D
```

---

### 实测性能

#### 测试环境
- **模型配置**: d_model=256, n_layer=4, d_inner=512, d_state=16
- **输入**: batch_size=1, seq_length=128
- **测试次数**: 10次取平均
- **硬件**: CPU（具体型号未指定）

#### 性能数据

| 版本 | 平均时间 | 最小时间 | 最大时间 |
|------|---------|---------|---------|
| **原始版本** | 31.59 ms | - | - |
| **优化版本** | 14.47 ms | - | - |

#### 性能提升

```
加速比: 2.18x
性能提升: 54.2%
时间减少: 17.12 ms
```

**结论**: 单纯通过算法重构，在不引入 C++/SIMD 的情况下，即可实现 **2.18倍加速**！

---

### 性能提升分析

#### 1. 消除循环内的函数调用开销

**原始版本的开销**：
- 128 次 `einsum` 调用（每次处理单个时间步）
- 128 次 `list.append` 操作（可能触发内存重新分配）
- 1 次 `torch.stack` 操作（重新组织内存）

**优化版本的改进**：
- 1 次大规模 `einsum` 调用（批量处理所有时间步）
- 无 list 操作，直接生成最终张量
- einsum 的启动开销和 Python 调度开销被分摊到更大的计算量

**收益估算**：
```
函数调用减少: 128 → 1 (减少 99.2%)
内存重组次数: 129 → 1 (减少 99.2%)
```

#### 2. 向量化效率提升

**小批量 vs 大批量 einsum**：

| 维度 | 原始（128次小einsum） | 优化（1次大einsum） |
|------|---------------------|-------------------|
| 单次输入形状 | `(1, 512, 16)` × `(1, 16)` | `(1, 128, 512, 16)` × `(1, 128, 16)` |
| 数据量 | 8KB/次 × 128 = 1MB | 1MB一次性处理 |
| 向量化效果 | 受限于小数据量 | 编译器/框架可深度优化 |
| 内存预取 | 每次重新加载 | 连续访问，预取效率高 |

PyTorch 的 einsum 在处理大批量数据时能更好利用：
- **CPU 向量化指令**（即使未显式使用 SIMD）
- **编译器优化**（循环展开、指令重排）
- **内存预取**（硬件层面的优化）

#### 3. 内存访问模式对比

**原始版本的内存流动**：
```
迭代 i:
  1. 读取 x (4KB, L1 cache) ✓
  2. 读取 deltaA[:, i] (4KB, 主存)
  3. 读取 deltaB_u[:, i] (4KB, 主存)
  4. 计算更新 x (写入 L1 cache) ✓
  5. 读取 C[:, i, :] (64B, 主存)
  6. einsum 计算 (多次内存访问)
  7. append 到 ys (可能触发内存分配)

总访存: ~8KB 主存读 + 4KB cache 读写 (×128次)
```

**优化版本的内存流动**：
```
阶段 1 (递推):
  迭代 i:
    1. 读取 deltaA[:, i] (4KB)
    2. 读取 deltaB_u[:, i-1] (4KB)
    3. 读取 deltaB_u[:, i] (4KB)
    4. 写入 deltaB_u[:, i] (4KB)
  总访存: ~16KB × 127 ≈ 2MB

阶段 2 (批量 einsum):
  1. 顺序读取 deltaB_u 全部 (512KB)
  2. 顺序读取 C 全部 (~8KB)
  3. 写入 y (64KB)
  总访存: ~584KB (顺序访问)

总计: 2MB + 584KB ≈ 2.6MB
```

**似乎更多？为何更快？**

关键在于：
1. **大块顺序访问 >> 小块随机访问**
   - 阶段 2 的 einsum 是连续内存访问，CPU 预取命中率极高
   - 原始版本每次都要跳转到不同的内存位置

2. **函数调用开销**
   - 原始版本：128 次进入 einsum
   - 优化版本：1 次进入 einsum
   - PyTorch 函数调用开销包括：参数检查、设备检查、形状推断等

3. **编译器优化空间**
   - 大块计算给了编译器更多优化机会
   - 小块计算受限于 Python 解释器开销

#### 4. 意外收益：缓存局部性

虽然递推循环访问 `deltaB_u[:, i-1]` 和 `deltaB_u[:, i]` 是跨时间步的：
- 但相邻时间步在内存中也是相邻的（stride = d_in × n = 8KB）
- 现代 CPU 的 L2/L3 缓存足够容纳多个时间步
- 顺序递推访问模式对硬件预取器友好

#### 5. Python 解释器开销

**原始版本**：
```python
for i in range(128):  # Python 循环
    # 4 次张量操作（每次都有 Python 开销）
    x = deltaA[:, i] * x + deltaB_u[:, i]
    y = einsum(...)  # 进入 C++ 扩展
    ys.append(y)     # Python list 操作
```

**优化版本**：
```python
for i in range(127):  # Python 循环
    # 1 次张量操作
    deltaB_u[:, i] = deltaA[:, i] * deltaB_u[:, i-1] + deltaB_u[:, i]

# 单次大规模 einsum，长时间在 C++ 层执行
y = einsum(deltaB_u, C, ...)
```

Python 解释器开销减少约 **3倍**（每次循环从 4 个操作降到 1 个）。

---

### 核心优化原则验证

这个实验证明了几个关键的性能优化原则：

#### ✅ 1. 批量化 > 逐个处理
```
128 次小 einsum → 1 次大 einsum = 2.18x 加速
```

#### ✅ 2. 函数调用开销不可忽视
对于 PyTorch 这类框架：
- 函数调用本身有开销（参数检查、分发等）
- 频繁进出 Python/C++ 边界代价高
- 应尽量让计算停留在高效层（C++/CUDA）

#### ✅ 3. 内存分配策略很重要
```python
# 差：动态增长
ys = []
for i in range(128):
    ys.append(y)  # 可能多次 realloc

# 好：一次性分配
y = einsum(...)  # 直接生成目标形状
```

#### ✅ 4. 简单的算法重构可以很强大
无需：
- ❌ C++ 扩展
- ❌ SIMD 指令
- ❌ 多线程并行

仅需：
- ✅ 重新组织计算顺序
- ✅ 批量化操作
- ✅ 减少函数调用

即可获得 **2x 加速**！

---

### 进一步优化潜力

基于这个成功案例，后续优化方向：

#### 短期（1-2天）
1. **C++ 实现递推循环**
   - 当前递推循环仍在 Python 层（127次迭代）
   - 用 PyTorch C++ 扩展实现，预计额外 1.3-1.5x 提升
   - **预期总加速**：2.18 × 1.4 ≈ **3.0x**

2. **融合算子**
   - 将递推循环和 einsum 融合为单个 CUDA/C++ kernel
   - 避免中间结果写回主存
   - **预期额外提升**：1.2-1.3x

#### 中期（3-5天）
3. **并行扫描（Parallel Scan）**
   - 当前递推是串行的（必须等 i-1 完成才能算 i）
   - 使用 prefix sum 风格的并行算法
   - 需要算法重新设计，但可实现 O(log L) 复杂度
   - **预期提升**：1.5-2.0x

4. **内存布局优化**
   - 转置张量维度使时间步连续
   - 当前：`(B, L, D, N)`，L 不连续
   - 优化后：`(B, D, L, N)`，L 连续
   - **预期提升**：1.2-1.3x

#### 组合效果预估
```
当前：2.18x
+ C++ 扩展：×1.4 = 3.0x
+ 融合算子：×1.25 = 3.8x
+ Parallel Scan：×1.5 = 5.7x
+ 内存布局：×1.2 = 6.8x

总预期：6-7x 加速（相对原始版本）
```

---

### 经验总结

#### 成功要素
1. **深入理解计算流程**：发现 x 和 y 的计算可以分离
2. **识别真正的瓶颈**：函数调用和内存分配，而非计算本身
3. **大胆尝试**：即使理论分析认为"跨步访问可能更慢"，实测证明批量化收益更大
4. **实测为准**：优化效果远超预期（预估 1.2x，实测 2.18x）

#### 教训
1. **不要过早优化细节**：没必要先用 C++ 重写，Python 层面就有大幅提升空间
2. **批量化是王道**：在深度学习框架中，批量操作几乎总是更快
3. **测量驱动优化**：使用 VizTracer 等工具精确定位瓶颈

---

### 代码实现

完整实现见：
- 优化版本：[`model_new.py`](model_new.py)（特别是 [`selective_scan()`](model_new.py:275) 方法）
- 性能测试：[`test.py`](test.py)（包含原始版本和优化版本的对比）
- 性能分析报告：
  - `mamba_profile.html`（原始版本）
  - `mamba_new_profile.html`（优化版本）

---

### 结论

通过**两阶段计算**的简单重构，在纯 PyTorch 层面实现了 **2.18倍加速**，性能提升 **54.2%**。

这验证了设计文档中的核心理念：
> **算法重构 > 底层优化**（至少在初期阶段）

后续结合 C++ 扩展和并行化技术，有望达到 **6-8倍**的总体加速效果。
