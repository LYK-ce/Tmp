#Presented by KeJi
#Date: 2025-12-20

# Mamba SSM 核心公式推导与代码对应

## 一、状态空间模型 (SSM) 基础理论

### 1.1 连续时间状态空间模型

**标准形式**:
```
dx(t)/dt = A·x(t) + B·u(t)    # 状态方程
y(t) = C·x(t) + D·u(t)        # 输出方程
```

**符号说明**:
- `x(t)`: 隐藏状态 (维度 n)
- `u(t)`: 输入信号 (维度 d_in)
- `y(t)`: 输出信号 (维度 d_in)
- `A`: 状态转移矩阵 (n × n)
- `B`: 输入矩阵 (n × d_in)
- `C`: 输出矩阵 (d_in × n)
- `D`: 直通矩阵 (d_in × d_in)

---

### 1.2 离散化 (Zero-Order Hold, ZOH)

**为什么需要离散化？**
- 计算机只能处理离散时间序列
- 需要将连续微分方程转换为递推公式

**离散化公式** (步长 Δt):
```
x[k+1] = Ā·x[k] + B̄·u[k]
y[k] = C·x[k] + D·u[k]

其中:
Ā = exp(A·Δt)           # 离散化的状态转移矩阵
B̄ = (Ā - I)·A⁻¹·B      # 离散化的输入矩阵 (简化为 Δt·B)
```

---

## 二、Mamba 的创新: Selective SSM

### 2.1 传统 SSM vs Mamba

#### 传统 SSM (如 S4)
```
A, B, C, D 都是**固定参数**，与输入无关
→ 限制了模型的表达能力
```

#### Mamba 的突破
```
Δt, B, C 是**输入依赖的**！
→ 不同输入有不同的参数
→ 这就是"Selective" (选择性) 的含义
```

**Mamba 的离散化公式**:
```
Δt = Parameter(x)      # 时间步长是输入的函数
B = Parameter(x)        # 输入矩阵是输入的函数
C = Parameter(x)        # 输出矩阵是输入的函数
A = 固定参数            # 只有 A 是固定的

Ā = exp(Δt ⊙ A)        # ⊙ 表示广播乘法
B̄ = Δt ⊙ B
```

---

## 三、代码中的两行公式解析

### 3.1 第一行: 状态更新

#### 代码: [`model.py:318`](model.py:318)
```python
x = deltaA[:, i] * x + deltaB_u[:, i]
```

#### 对应的数学公式
```
x[i] = Ā[i] ⊙ x[i-1] + B̄[i] ⊙ u[i]

其中:
Ā[i] = exp(Δt[i] ⊙ A)     # deltaA[:, i]
B̄[i]·u[i] = Δt[i] ⊙ B[i] ⊙ u[i]  # deltaB_u[:, i]
```

#### 详细推导

**Step 1**: 计算离散化的 A
```python
# 代码: model.py:309
deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))
         #        ^^^^^^^^ 对应公式: exp(Δt ⊙ A)

# delta: (b, l, d_in) = (1, 128, 256)  # Δt[i] for each timestep
# A:     (d_in, n) = (512, 16)         # 固定的状态矩阵
# 输出:  (b, l, d_in, n) = (1, 128, 256, 16)  # Ā[i] for each timestep
```

**数学解释**:
```
对于每个时间步 i、每个通道 d、每个状态维度 n:
Ā[i, d, n] = exp(Δt[i, d] × A[d, n])

这实现了"选择性"：
- 不同时间步有不同的状态转移
- 不同通道有不同的转移速度
```

**Step 2**: 计算离散化的 B·u
```python
# 代码: model.py:310
deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')
           # 对应公式: Δt ⊙ B ⊙ u

# delta: (1, 128, 256)  # Δt
# B:     (1, 128, 16)   # 输入矩阵
# u:     (1, 128, 256)  # 输入信号
# 输出:  (1, 128, 256, 16)  # B̄·u for each timestep
```

**数学解释**:
```
对于每个时间步 i、每个通道 d、每个状态维度 n:
(B̄·u)[i, d, n] = Δt[i, d] × B[i, n] × u[i, d]

注意: 这里的 B 对每个通道 d 是共享的，
但通过 Δt[i, d] 和 u[i, d] 实现了选择性
```

**Step 3**: 递推更新状态
```python
# 代码: model.py:318
x = deltaA[:, i] * x + deltaB_u[:, i]
#   ^^^^^^^^^^^^^^   ^^^^^^^^^^^^^
#   状态衰减         输入贡献
```

**对应 Mamba 论文 Algorithm 2**:
```
h[t] = Ā[t] ⊙ h[t-1] + B̄[t] ⊙ u[t]
     = exp(Δ[t] ⊙ A) ⊙ h[t-1] + (Δ[t] ⊙ B[t]) ⊙ u[t]
```

**维度追踪**:
```
x (状态):           (b, d_in, n) = (1, 256, 16)  # 上一时刻的状态
deltaA[:, i]:       (b, d_in, n) = (1, 256, 16)  # 第 i 步的 Ā
deltaA[:, i] * x:   (b, d_in, n) = (1, 256, 16)  # 状态衰减
deltaB_u[:, i]:     (b, d_in, n) = (1, 256, 16)  # 第 i 步的 B̄·u
x (新状态):         (b, d_in, n) = (1, 256, 16)  # 更新后的状态
```

---

### 3.2 第二行: 输出映射

#### 代码: [`model.py:319`](model.py:319)
```python
y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')
```

#### 对应的数学公式
```
y[i] = C[i] · x[i]
```

#### 详细推导

**数学表达**:
```
y[i] = C[i] · h[i]

其中:
C[i]: (b, n) = (1, 16)       # 第 i 步的输出矩阵
h[i]: (b, d_in, n) = (1, 256, 16)  # 第 i 步的隐藏状态
y[i]: (b, d_in) = (1, 256)   # 第 i 步的输出
```

**einsum 展开**:
```python
# einsum 格式: 'b d_in n, b n -> b d_in'
# 
# 对于每个 batch b、每个通道 d_in:
# y[b, d_in] = Σ(n=0 to 15) x[b, d_in, n] × C[b, n]
#
# 这是一个矩阵-向量乘法:
# y = x @ C
```

**维度追踪**:
```
x:              (b, d_in, n) = (1, 256, 16)  # 当前状态
C[:, i, :]:     (b, n) = (1, 16)             # 第 i 步的输出权重
y:              (b, d_in) = (1, 256)         # 输出向量
```

**物理意义**:
- `x` 是 256 个通道，每个通道有 16 维隐藏状态
- `C` 是将这 16 维状态映射回 1 维输出
- 最终得到 256 个通道的输出

---

## 四、完整的数学公式链

### 4.1 从输入到输出的完整流程

#### 准备阶段 (在循环外)

**1. 计算输入依赖参数**
```python
# 代码: model.py:265-268
x_dbl = self.x_proj(x)                    # x → [Δ, B, C]
(delta, B, C) = x_dbl.split(...)
delta = F.softplus(self.dt_proj(delta))   # 确保 Δ > 0
```

**数学**:
```
Δ = softplus(W_Δ · x + b_Δ)
B = W_B · x
C = W_C · x
```

**2. 离散化参数**
```python
# 代码: model.py:309-310
deltaA = torch.exp(einsum(delta, A, ...))      # Ā = exp(Δ ⊙ A)
deltaB_u = einsum(delta, B, u, ...)            # B̄·u = Δ ⊙ B ⊙ u
```

**数学**:
```
Ā[i] = exp(Δ[i] ⊙ A)      # 对每个时间步 i
B̄[i]·u[i] = Δ[i] ⊙ B[i] ⊙ u[i]
```

---

#### 循环阶段 (递推计算)

**循环 i = 0 到 127**:

**Line 318: 状态更新**
```python
x = deltaA[:, i] * x + deltaB_u[:, i]
```

**对应 Mamba 论文 Algorithm 2, Line 9**:
```
h[t] = Ā[t] ⊙ h[t-1] + B̄[t]·u[t]
```

**数学推导**:
```
从连续公式:  dx/dt = A·x + B·u
使用 ZOH 离散化:
x[i] = exp(A·Δt[i])·x[i-1] + (∫₀^Δt exp(A·τ)dτ)·B·u[i]

简化 (Mamba 使用的近似):
x[i] = exp(Δt[i]⊙A) ⊙ x[i-1] + Δt[i]⊙B[i]⊙u[i]
     = Ā[i] ⊙ x[i-1] + B̄[i]·u[i]
```

**代码映射**:
```python
x      ←→  h[t]          # 当前状态
deltaA ←→  Ā[t]          # exp(Δ[t] ⊙ A)
deltaB_u ←→ B̄[t]·u[t]    # Δ[t] ⊙ B[t] ⊙ u[t]
```

---

**Line 319: 输出计算**
```python
y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')
```

**对应 Mamba 论文 Algorithm 2, Line 10**:
```
y[t] = C[t] · h[t]
```

**数学**:
```
y[i] = C[i] · x[i]
     = C[i] · (Ā[i] ⊙ x[i-1] + B̄[i]·u[i])
```

**代码映射**:
```python
y       ←→  y[t]    # 当前时间步的输出
x       ←→  h[t]    # 刚更新的状态
C[:, i] ←→  C[t]    # 第 i 步的输出矩阵
```

---

## 五、Mamba 论文对应

### 5.1 论文 Algorithm 2 (Selective Scan)

```
输入: u (b, l, d_in), Δ (b, l, d_in), A (d_in, n), B (b, l, n), C (b, l, n)

1-7: 参数计算和离散化
8:   h ← 0
9:   for t = 1 to l do
10:      h ← Ā[t] ⊙ h + B̄[t]·u[t]     ← Line 318
11:      y[t] ← C[t] · h                ← Line 319
12:  end for

输出: y (b, l, d_in)
```

### 5.2 代码与论文的精确对应

| 论文符号 | 论文描述 | 代码变量 | 代码位置 |
|---------|---------|---------|---------|
| `u` | 输入序列 | `u` | 函数参数 |
| `Δ` | 时间步长 | `delta` | Line 268 |
| `A` | 状态矩阵 | `A` | Line 262 |
| `B` | 输入矩阵 | `B` | Line 267 |
| `C` | 输出矩阵 | `C` | Line 267 |
| `Ā[t]` | 离散化A | `deltaA[:, i]` | Line 309->318 |
| `B̄[t]·u[t]` | 离散化B·u | `deltaB_u[:, i]` | Line 310->318 |
| `h` | 隐藏状态 | `x` | Line 315, 318 |
| `y[t]` | 输出 | `y` | Line 319 |

---

## 六、直观理解

### 6.1 状态更新 (Line 318) 的物理意义

```python
x = deltaA[:, i] * x + deltaB_u[:, i]
    ^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^
    "遗忘旧信息"      "加入新信息"
```

#### 类比: RNN 的隐藏状态更新

**传统 RNN**:
```
h[t] = tanh(W_h·h[t-1] + W_x·x[t])
       ^^^^^^^^^^^^    ^^^^^^^^^^
       旧状态的变换    新输入的贡献
```

**Mamba SSM**:
```
h[t] = exp(Δ[t]⊙A) ⊙ h[t-1] + Δ[t]⊙B[t]⊙u[t]
       ^^^^^^^^^^^^^^           ^^^^^^^^^^^^^^
       "遗忘因子"               "记忆新输入"
       (动态控制记忆多少)        (动态控制接收多少)
```

#### 选择性的体现

**固定 SSM (S4)**:
```
Ā 是固定的 → 所有输入用相同的遗忘率
```

**Mamba (Selective SSM)**:
```
Ā[i] = exp(Δ[i] ⊙ A) → Δ[i] 根据输入 u[i] 动态调整
  ↓
重要输入: Δ 大 → Ā 大 → 记住更多
不重要输入: Δ 小 → Ā 小 → 快速遗忘
```

---

### 6.2 输出映射 (Line 319) 的物理意义

```python
y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')
```

#### 数学意义

**从高维状态到低维输出**:
```
隐藏状态 x: (1, 256, 16)  # 256个通道,每个有16维状态
                          ↓ 线性映射 (C)
输出 y:     (1, 256)      # 256个通道,每个1维输出
```

**选择性的体现**:
```
C[i] 是输入依赖的 → 不同时间步关注状态的不同部分

例如:
- 处理重要token: C[i] 可能均匀权重所有 16 维
- 处理padding: C[i] 可能权重接近0,忽略状态
```

---

## 七、完整的数学推导示例

### 假设具体数值，理解计算过程

#### 初始状态
```
时间步 i = 0
输入 u[0] = [0.5, 0.3, ...]       (256维)
上一状态 x[-1] = 0                 (256×16 全零)

参数:
Δ[0] = [0.1, 0.2, ...]            (256维)
A = [[-1, -2, ...], ...]          (256×16)
B[0] = [1.5, 2.0, ...]            (16维)
C[0] = [0.8, 0.9, ...]            (16维)
```

#### Step 1: 计算 Ā[0]
```python
# 对于第0个通道 (d=0):
deltaA[0, 0] = exp(Δ[0, 0] × A[0, :])
             = exp(0.1 × [-1, -2, -3, ..., -16])
             = [exp(-0.1), exp(-0.2), ..., exp(-1.6)]
             = [0.905, 0.819, 0.741, ..., 0.202]

# 这是一个"遗忘向量"
# 值越小,遗忘越多
```

#### Step 2: 计算 B̄[0]·u[0]
```python
# 对于第0个通道 (d=0):
deltaB_u[0, 0] = Δ[0, 0] × B[0] × u[0, 0]
               = 0.1 × [1.5, 2.0, ...] × 0.5
               = [0.075, 0.100, ...]

# 这是"新信息的贡献"
```

#### Step 3: 状态更新 (Line 318)
```python
# 第0个通道的状态更新:
x[0] = deltaA[0, 0] * x[-1, 0] + deltaB_u[0, 0]
     = [0.905, 0.819, ...] * [0, 0, ...] + [0.075, 0.100, ...]
     = [0.075, 0.100, ...]  (因为初始状态是0)

# 从第1步开始:
x[1] = deltaA[1, 0] * x[0, 0] + deltaB_u[1, 0]
     = [0.9, 0.8, ...] * [0.075, 0.100, ...] + [新输入]
     = [0.0675, 0.080, ...] + [新输入]
     = [旧信息衰减] + [新信息]
```

#### Step 4: 输出计算 (Line 319)
```python
# 第0个通道的输出:
y[0, 0] = C[0] · x[0]
        = [0.8, 0.9, 0.7, ..., 0.5] · [0.075, 0.100, ..., 0.2]
        = 0.8×0.075 + 0.9×0.100 + ... + 0.5×0.2
        = 标量输出
```

---

## 八、为什么需要这样计算？

### 8.1 递推的必要性

**问题**: 为什么不能一次性计算所有时间步？

**答案**: 因为**递推依赖**
```
x[0] = Ā[0] ⊙ x[-1] + B̄[0]·u[0]
x[1] = Ā[1] ⊙ x[0] + B̄[1]·u[1]    ← 依赖 x[0]
x[2] = Ā[2] ⊙ x[1] + B̄[2]·u[2]    ← 依赖 x[1]
...

每一步都依赖前一步的结果，无法并行！
```

这就是为什么**必须用 for 循环**，也是为什么**在 CPU 上很慢**。

---

### 8.2 与其他序列模型的对比

#### Transformer (Self-Attention)
```
y[i] 可以直接从 u[0:i] 计算
→ 所有时间步可以并行
→ 但计算复杂度 O(L²)
```

#### RNN/LSTM
```
h[i] 依赖 h[i-1]
→ 必须串行计算
→ 但参数是固定的 (非选择性)
```

#### Mamba (Selective SSM)
```
h[i] 依赖 h[i-1]
→ 必须串行计算
但: Ā[i], B[i], C[i] 是输入依赖的
→ 更强的表达能力
→ 线性复杂度 O(L)
```

---

## 九、代码中的技巧和优化

### 9.1 为什么用 einsum？

**Line 319 的 einsum**:
```python
y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')
```

**优点**:
- **灵活性**: 清晰表达矩阵运算的维度关系
- **可读性**: '格式字符串比多个 reshape + matmul 更清晰

**缺点**:
- **性能**: 小矩阵时，einsum 不如手动优化的 matmul
- **开销**: 需要解析格式字符串

**等价但可能更快的写法**:
```python
# 方法1: 使用 matmul
y = torch.matmul(x, C[:, i, :].unsqueeze(-1)).squeeze(-1)

# 方法2: 使用 bmm (batch matmul)
y = torch.bmm(x.view(b*d_in, 1, n), C[:, i, :].unsqueeze(1)).squeeze(1)

# 方法3: 手动 sum
y = torch.sum(x * C[:, i, :].unsqueeze(1), dim=2)
```

---

### 9.2 为什么 A 是负数？

**代码**: [`model.py:262`](model.py:262)
```python
A = -torch.exp(self.A_log.float())  # 注意负号！
```

**数学原因**:
```
dx/dt = A·x + B·u

如果 A > 0: 状态会指数增长 → 数值不稳定
如果 A < 0: 状态会衰减 → 稳定的"遗忘"机制

Ā = exp(Δt·A) 
  = exp(Δt·负数)
  = 0 到 1 之间的值
  → 作为"遗忘门"
```

**直观理解**:
```
A = -1:  Ā = exp(-0.1) ≈ 0.90  → 保留 90% 的旧状态
A = -10: Ā = exp(-1.0) ≈ 0.37  → 保留 37% 的旧状态

Δt 越大: Ā 越小 → 遗忘越多
Δt 越小: Ā 越大 → 记忆越多
```

---

## 十、总结：两行代码的完整含义

### Line 318: `x = deltaA[:, i] * x + deltaB_u[:, i]`

**数学公式**: 
```
h[t] = exp(Δ[t]⊙A) ⊙ h[t-1] + Δ[t]⊙B[t]⊙u[t]
```

**含义**:
- 更新第 i 个时间步的隐藏状态
- **选择性遗忘**: `deltaA * x` 根据输入重要性动态调整遗忘率
- **选择性记忆**: `deltaB_u` 根据输入重要性动态调整吸收率

**在 Mamba 中的作用**:
- 这是模型"压缩历史信息"的核心机制
- 通过 Δ[i] 动态控制信息流

---

### Line 319: `y = einsum(x, C[:, i, :], ...)`

**数学公式**: 
```
y[t] = C[t] · h[t]
```

**含义**:
- 从隐藏状态提取输出
- **选择性读取**: C[t] 动态决定关注状态的哪些维度

**在 Mamba 中的作用**:
- 将压缩的状态信息解码为输出
- 通过 C[i] 动态选择关注的信息

---

## 十一、完整的信息流

```
输入 u[i]
   ↓
┌──────────────────────────────────────────┐
│ 计算选择性参数                            │
│ Δ[i], B[i], C[i] = f(u[i])              │ ← x_proj, dt_proj
└──────────────────────────────────────────┘
   ↓
┌──────────────────────────────────────────┐
│ 离散化                                    │
│ Ā[i] = exp(Δ[i] ⊙ A)                    │ ← deltaA 计算
│ B̄[i]·u[i] = Δ[i] ⊙ B[i] ⊙ u[i]         │ ← deltaB_u 计算
└──────────────────────────────────────────┘
   ↓
┌──────────────────────────────────────────┐
│ 🔴 状态更新 (Line 318)                   │
│ h[i] = Ā[i] ⊙ h[i-1] + B̄[i]·u[i]       │
│                                          │
│ 语义: 选择性地                            │
│ - 遗忘旧信息 (Ā[i] ⊙ h[i-1])            │
│ - 吸收新信息 (B̄[i]·u[i])                │
└──────────────────────────────────────────┘
   ↓
┌──────────────────────────────────────────┐
│ 输出映射 (Line 319)                      │
│ y[i] = C[i] · h[i]                       │
│                                          │
│ 语义: 选择性地从状态提取输出              │
└──────────────────────────────────────────┘
   ↓
输出 y[i]
```

---

## 十二、Mamba vs LSTM 的对比

### 相似之处

**LSTM**:
```
f[t] = σ(W_f·[h[t-1], x[t]])    # 遗忘门
i[t] = σ(W_i·[h[t-1], x[t]])    # 输入门
c[t] = f[t]⊙c[t-1] + i[t]⊙tanh(W_c·[h[t-1], x[t]])
       ^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^
       遗忘旧状态   记忆新信息
```

**Mamba**:
```
h[t] = exp(Δ[t]⊙A) ⊙ h[t-1] + Δ[t]⊙B[t]⊙u[t]
       ^^^^^^^^^^^^           ^^^^^^^^^^^^^^
       "连续遗忘门"           "连续输入门"
```

### 关键区别

| 特性 | LSTM | Mamba |
|------|------|-------|
| 遗忘机制 | 离散门控 (0-1) | 连续衰减 (exp) |
| 参数化 | 门参数固定维度 | 状态空间参数 |
| 计算复杂度 | O(d²) | O(d·n), n << d |
| 并行性 | 必须串行 | 理论可并行* |

*注: Mamba 在 GPU 上通过 parallel scan 实现并行

---

## 十三、为什么这样设计？

### 13.1 相比 Transformer 的优势

**Transformer**:
```
y[i] = Attention(Q, K, V)[i]
     = softmax(Q[i]·K^T/√d) · V
     
复杂度: O(L²·d)  # L=序列长度
→ 长序列时很慢
```

**Mamba**:
```
h[i] = Ā[i]⊙h[i-1] + B̄[i]·u[i]
y[i] = C[i]·h[i]

复杂度: O(L·d·n)  # n << d, 通常 n=16, d=256
→ 线性复杂度！
```

---

### 13.2 相比传统 SSM (S4) 的优势

**S4 (固定 SSM)**:
```
A, B, C 都是固定的
→ 无法根据输入动态调整
→ 表达能力受限
```

**Mamba (选择性 SSM)**:
```
Δ, B, C 是输入依赖的
→ 可以"选择性"处理输入
→ 更强的建模能力

例如:
- 重要token: Δ大，B大 → 强烈吸收
- padding token: Δ小，B小 → 几乎忽略
```

---

## 十四、优化不改变数学本质

### 重要提醒

**当前代码**:
```python
x = deltaA[:, i] * x + deltaB_u[:, i]
y = einsum(x, C[:, i, :], ...)
```

**优化后代码**:
```python
x = torch.addcmul(deltaB_u[:, i], deltaA[:, i], x)  # 融合
y = torch.matmul(x, C[:, i].unsqueeze(-1)).squeeze(-1)  # 替代einsum
```

**数学公式完全相同**！
```
h[t] = Ā[t] ⊙ h[t-1] + B̄[t]·u[t]
y[t] = C[t] · h[t]

优化只是改变了"计算方式"，不改变"计算内容"
```

---

**总结**: 
- **Line 318** = Mamba 的**状态更新方程**（核心递推）
- **Line 319** = Mamba 的**输出方程**（状态投影）
- 这两行实现了 Mamba 论文 Algorithm 2 的第 10-11 行
- 优化这两行就是优化 Mamba 最核心的计算！
