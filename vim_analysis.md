#Presented by KeJi
#Date: 2025-12-22

# Vision Mamba è®¡ç®—æµç¨‹åˆ†æ

## ä¸€ã€Vision Mamba æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ€æƒ³
Vision Mamba (Vim) æ˜¯å°† Mamba SSM æ¶æ„åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„åˆ›æ–°æ¨¡å‹ï¼Œé€šè¿‡**åŒå‘çŠ¶æ€ç©ºé—´æ¨¡å‹**å¤„ç†å›¾åƒpatchåºåˆ—ï¼Œå®ç°é«˜æ•ˆçš„è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… çº¿æ€§æ—¶é—´å¤æ‚åº¦ O(L)ï¼ˆç›¸æ¯”Transformerçš„ O(LÂ²)ï¼‰
- âœ… åŒå‘æ‰«ææœºåˆ¶æ•è·å…¨å±€ä¸Šä¸‹æ–‡
- âœ… é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†æ•ˆç‡é«˜ï¼ˆ2.8x faster than DeiTï¼‰
- âœ… å†…å­˜æ•ˆç‡æ˜¾è‘—ï¼ˆèŠ‚çœ 86.8% GPU å†…å­˜ï¼Œ1248Ã—1248 åˆ†è¾¨ç‡ï¼‰

### 1.2 æ¨¡å‹è§„æ¨¡

| æ¨¡å‹ | å‚æ•°é‡ | ImageNet Top-1 | ç”¨é€” |
|------|--------|----------------|------|
| Vim-Tiny | 7M | 76.1% / 78.3%+ | è½»é‡çº§åº”ç”¨ |
| Vim-Small | 26M | 80.5% / 81.6%+ | å¹³è¡¡æ€§èƒ½ |
| Vim-Base | 98M | 81.9% | é«˜æ€§èƒ½åº”ç”¨ |

*æ³¨ï¼š+è¡¨ç¤ºfiner granularityå¾®è°ƒç‰ˆæœ¬*

---

## äºŒã€Vision Mamba è®¡ç®—æµç¨‹è¯¦è§£

### 2.1 æ•´ä½“æ¶æ„æµç¨‹

```
è¾“å…¥å›¾åƒ (B, 3, H, W)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Patch Embeddingï¼ˆå›¾åƒåˆ†å—ï¼‰            â”‚
â”‚    Conv2d(3, embed_dim, kernel=16, stride=16) â”‚
â”‚    è¾“å‡º: (B, num_patches, embed_dim)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. CLS Token æ’å…¥ï¼ˆå¯é€‰ï¼‰                 â”‚
â”‚    ä½ç½®: å¼€å§‹/ä¸­é—´/æœ«å°¾                   â”‚
â”‚    è¾“å‡º: (B, num_patches+1, embed_dim)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ä½ç½®ç¼–ç                                â”‚
â”‚    - ç»å¯¹ä½ç½®ç¼–ç  or                      â”‚
â”‚    - RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Dropout                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. å †å çš„ Mamba Blocksï¼ˆÃ—24å±‚ï¼‰          â”‚
â”‚    æ¯ä¸ªBlockï¼š                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚    â”‚ a. æ®‹å·®è¿æ¥                    â”‚   â”‚
â”‚    â”‚ b. Layer Norm                  â”‚   â”‚
â”‚    â”‚ c. Mamba Mixerï¼ˆåŒå‘æ‰«æï¼‰     â”‚   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Final Layer Norm                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. ç‰¹å¾æå–                               â”‚
â”‚    - CLS tokenç‰¹å¾ or                     â”‚
â”‚    - Global Average Pooling              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. åˆ†ç±»å¤´ï¼ˆLinearï¼‰                       â”‚
â”‚    è¾“å‡º: (B, num_classes)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.2 å…³é”®æ¨¡å—è¯¦ç»†åˆ†æ

#### 2.2.1 Patch Embedding

**ä»£ç ä½ç½®**: [`models_mamba.py:42-66`](VisionMamba_CPU/vim/models_mamba.py:42)

```python
class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, stride=16, 
                 in_chans=3, embed_dim=768):
        # ä½¿ç”¨å·ç§¯å®ç°patchåˆ†å‰²
        self.proj = nn.Conv2d(in_chans, embed_dim, 
                             kernel_size=patch_size, stride=stride)
```

**è®¡ç®—è¿‡ç¨‹**ï¼š
```
è¾“å…¥å›¾åƒ: (1, 3, 224, 224)
    â†“ Conv2d(3â†’192, kernel=16, stride=16)
Flattened: (1, 192, 14, 14)
    â†“ flatten(2).transpose(1, 2)
Patches: (1, 196, 192)  # 14Ã—14=196ä¸ªpatches
```

**ç‰©ç†æ„ä¹‰**ï¼š
- æ¯ä¸ª 16Ã—16 çš„å›¾åƒå— â†’ ä¸€ä¸ª 192 ç»´å‘é‡
- ç›¸å½“äºå°† 2D å›¾åƒè½¬æ¢ä¸º 1D åºåˆ—
- ä¿ç•™ç©ºé—´ç»“æ„çš„è¯­ä¹‰ä¿¡æ¯

---

#### 2.2.2 ä½ç½®ç¼–ç ç­–ç•¥

**æ–¹æ³•1: ç»å¯¹ä½ç½®ç¼–ç **ï¼ˆé»˜è®¤ï¼‰
```python
self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + num_tokens, embed_dim))
x = x + self.pos_embed  # ç›´æ¥åŠ æ³•
```

**æ–¹æ³•2: RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰**ï¼ˆå¯é€‰ï¼‰
```python
self.rope = VisionRotaryEmbeddingFast(
    dim=half_head_dim,
    pt_seq_len=pt_hw_seq_len,
    ft_seq_len=hw_seq_len
)
hidden_states = self.rope(hidden_states)  # åœ¨æ¯å±‚åº”ç”¨
```

**å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | ç»å¯¹ä½ç½®ç¼–ç  | RoPE |
|------|-------------|------|
| åº”ç”¨ä½ç½® | è¾“å…¥å±‚ä¸€æ¬¡ | æ¯ä¸ªMambaå±‚ |
| å¤–æ¨èƒ½åŠ› | è¾ƒå¼± | è¾ƒå¼º |
| è®¡ç®—å¼€é”€ | ä½ | ä¸­ç­‰ |
| ä½ç½®æ•æ„Ÿæ€§ | å…¨å±€ | ç›¸å¯¹ |

---

#### 2.2.3 CLS Token å¤„ç†

**ä¸‰ç§ä½ç½®ç­–ç•¥**ï¼š

```python
# 1. å¼€å§‹ä½ç½®ï¼ˆä¼ ç»ŸViTæ–¹å¼ï¼‰
x = torch.cat((cls_token, x), dim=1)  # token_position = 0

# 2. ä¸­é—´ä½ç½®ï¼ˆVimé»˜è®¤ï¼Œæ€§èƒ½æœ€ä¼˜ï¼‰
mid = M // 2
x = torch.cat((x[:, :mid, :], cls_token, x[:, mid:, :]), dim=1)

# 3. åŒtokenï¼ˆå¤´å°¾å„ä¸€ä¸ªï¼‰
x = torch.cat((cls_token_head, x, cls_token_tail), dim=1)
# æœ€ç»ˆè¾“å‡º: (feat_head + feat_tail) / 2
```

**ä¸ºä»€ä¹ˆä¸­é—´ä½ç½®æ›´å¥½ï¼Ÿ**
- âœ… **å¯¹ç§°æ€§**ï¼šå‰åæ–¹å‘æ‰«æè·ç¦»å‡è¡¡
- âœ… **ä¿¡æ¯æµ**ï¼šæ›´å®¹æ˜“èšåˆå…¨å±€ä¿¡æ¯
- âœ… **å®éªŒéªŒè¯**ï¼šTop-1å‡†ç¡®ç‡æå‡ 2.2%ï¼ˆ76.1% â†’ 78.3%ï¼‰

---

#### 2.2.4 Mamba Blockï¼ˆæ ¸å¿ƒè®¡ç®—ï¼‰

**ä»£ç ä½ç½®**: [`models_mamba.py:69-146`](VisionMamba_CPU/vim/models_mamba.py:69)

**Blockç»“æ„**ï¼š

```python
class Block(nn.Module):
    def forward(self, hidden_states, residual=None):
        # 1. æ®‹å·®è¿æ¥ + LayerNormï¼ˆfusedæˆ–åˆ†ç¦»ï¼‰
        if not self.fused_add_norm:
            residual = residual + hidden_states  # æ®‹å·®
            hidden_states = self.norm(residual)   # å½’ä¸€åŒ–
        else:
            # èåˆç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼‰
            hidden_states, residual = fused_add_norm_fn(...)
        
        # 2. Mambaæ··åˆå™¨ï¼ˆæ ¸å¿ƒSSMè®¡ç®—ï¼‰
        hidden_states = self.mixer(hidden_states, inference_params)
        
        return hidden_states, residual
```

**Mamba Mixer å†…éƒ¨æµç¨‹**ï¼š

```python
# ä»£ç ä½ç½®: mamba_ssm/modules/mamba_simple.py:186-325

def forward(self, hidden_states, inference_params=None):
    batch, seqlen, dim = hidden_states.shape  # (1, 197, 192)
    
    # 1. è¾“å…¥æŠ•å½± (åˆ†è£‚ä¸º x å’Œ z)
    xz = self.in_proj(hidden_states)  # (1, 197, 384) â†’ 2Ã—d_inner
    x, z = xz.chunk(2, dim=-1)        # å„ (1, 197, 192)
    
    # 2. æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆå±€éƒ¨ç‰¹å¾ï¼‰
    x = self.conv1d(x)  # (1, 192, 197) â†’ (1, 192, 197)
    x = self.act(x)     # SiLUæ¿€æ´»
    
    # 3. è®¡ç®—é€‰æ‹©æ€§å‚æ•° Î”, B, C
    x_dbl = self.x_proj(x)  # (1, 197, dt_rank + 2*d_state)
    dt, B, C = torch.split(x_dbl, [...], dim=-1)
    dt = self.dt_proj(dt)   # (1, 197, d_inner)
    
    # 4. Selective Scanï¼ˆæ ¸å¿ƒSSMé€’æ¨ï¼‰
    y = selective_scan_fn(
        x, dt, A, B, C, D, z=z,
        delta_bias=self.dt_proj.bias,
        delta_softplus=True
    )
    
    # 5. è¾“å‡ºæŠ•å½±
    out = self.out_proj(y)
    return out
```

---

#### 2.2.5 åŒå‘Mamba (BiMamba)

**BiMamba v2 æœºåˆ¶**ï¼ˆé»˜è®¤é…ç½®ï¼‰ï¼š

```python
# ä»£ç ä½ç½®: models_mamba.py:233-263

# å‰å‘æ‰«æ
out = mamba_inner_fn_no_out_proj(
    xz, conv1d.weight, ..., A, ...
)

# åå‘æ‰«æï¼ˆç¿»è½¬åºåˆ—ï¼‰
out_b = mamba_inner_fn_no_out_proj(
    xz.flip([-1]),        # ç¿»è½¬åºåˆ—
    conv1d_b.weight, ..., 
    A_b, ...
)

# åˆå¹¶å‰åå‘ç»“æœ
out_final = out + out_b.flip([-1])  # åå‘ç»“æœå†ç¿»è½¬å›æ¥
out_final = self.out_proj(out_final)
```

**åŒå‘æ‰«æç¤ºæ„å›¾**ï¼š

```
Patchåºåˆ—: [1][2][3][4][5]...[196]

å‰å‘æ‰«æ â†’: 
  [1] â†’ [2] â†’ [3] â†’ [4] â†’ [5] ... â†’ [196]
  hâ‚   hâ‚‚    hâ‚ƒ    hâ‚„    hâ‚…        hâ‚â‚‰â‚†

åå‘æ‰«æ â†:
  [196] â† [195] â† ... â† [3] â† [2] â† [1]
  h'â‚â‚‰â‚†  h'â‚â‚‰â‚…      h'â‚ƒ  h'â‚‚  h'â‚

æœ€ç»ˆè¾“å‡º: h + h'ï¼ˆé€å…ƒç´ ç›¸åŠ ï¼‰
```

**ä¸ºä»€ä¹ˆéœ€è¦åŒå‘ï¼Ÿ**
1. **å…¨å±€æ„Ÿå—é‡**ï¼šæ¯ä¸ªpatchåŒæ—¶çœ‹åˆ°å‰åæ‰€æœ‰patch
2. **å¯¹ç§°æ€§**ï¼šé¿å…å•å‘æ‰«æçš„ä½ç½®åå¥½
3. **æ€§èƒ½æå‡**ï¼šåŒå‘æ¯”å•å‘å‡†ç¡®ç‡æå‡çº¦1-2%
4. **è§†è§‰ä»»åŠ¡ç‰¹æ€§**ï¼šå›¾åƒç†è§£éœ€è¦å…¨æ–¹å‘çš„ä¸Šä¸‹æ–‡

---

### 2.3 Selective Scan æ ¸å¿ƒç®—æ³•

**ä»£ç ä½ç½®**: [`selective_scan_interface.py:132-198`](VisionMamba_CPU/mamba-1p1p1/mamba_ssm/ops/selective_scan_interface.py:132)

**æ•°å­¦å…¬å¼**ï¼š

```
ç»™å®šè¾“å…¥ u(t), è®¡ç®—è¾“å‡º y(t):

1. ç¦»æ•£åŒ–:
   Î” = softplus(Wáµ¨Â·x + báµ¨)           # æ—¶é—´æ­¥é•¿ï¼ˆè¾“å…¥ä¾èµ–ï¼‰
   Ä€ = exp(Î” âŠ™ A)                     # ç¦»æ•£åŒ–çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ
   BÌ„ = Î” âŠ™ B                          # ç¦»æ•£åŒ–çš„è¾“å…¥çŸ©é˜µ

2. é€’æ¨æ›´æ–°:
   x[t] = Ä€[t] âŠ™ x[t-1] + BÌ„[t] âŠ™ u[t]

3. è¾“å‡ºæ˜ å°„:
   y[t] = C[t] Â· x[t] + D Â· u[t]

4. é—¨æ§ï¼ˆå¦‚æœæœ‰zï¼‰:
   output[t] = y[t] âŠ™ SiLU(z[t])
```

**PyTorchå‚è€ƒå®ç°**ï¼š

```python
def selective_scan_ref(u, delta, A, B, C, D=None, z=None, 
                      delta_bias=None, delta_softplus=False):
    """
    u: (B, D, L) - è¾“å…¥åºåˆ—
    delta: (B, D, L) - æ—¶é—´æ­¥é•¿
    A: (D, N) - çŠ¶æ€çŸ©é˜µ
    B: (B, N, L) - è¾“å…¥çŸ©é˜µï¼ˆå˜åŒ–çš„ï¼‰
    C: (B, N, L) - è¾“å‡ºçŸ©é˜µï¼ˆå˜åŒ–çš„ï¼‰
    D: (D,) - è·³è·ƒè¿æ¥
    z: (B, D, L) - é—¨æ§ä¿¡å·
    """
    # 1. é¢„å¤„ç†delta
    if delta_bias is not None:
        delta = delta + delta_bias[..., None]
    if delta_softplus:
        delta = F.softplus(delta)
    
    # 2. è®¡ç®—ç¦»æ•£åŒ–å‚æ•°
    deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))
    deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)
    
    # 3. é€’æ¨è®¡ç®—éšè—çŠ¶æ€
    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]
    x = A.new_zeros((batch, dim, dstate))  # åˆå§‹åŒ–
    ys = []
    
    for i in range(u.shape[2]):  # éå†åºåˆ—é•¿åº¦
        # çŠ¶æ€æ›´æ–°
        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
        # è¾“å‡ºè®¡ç®—
        y = torch.einsum('bdn,bn->bd', x, C[:, :, i])
        ys.append(y)
    
    # 4. å †å è¾“å‡º
    y = torch.stack(ys, dim=2)  # (B, D, L)
    
    # 5. è·³è·ƒè¿æ¥
    if D is not None:
        y = y + u * D[..., None]
    
    # 6. é—¨æ§
    if z is not None:
        y = y * F.silu(z)
    
    return y
```

**å…³é”®ä¼˜åŒ–ç‚¹**ï¼ˆç›¸æ¯”æ ‡å‡†å®ç°ï¼‰ï¼š
- âœ… **é¢„è®¡ç®— `deltaA` å’Œ `deltaB_u`**ï¼šé¿å…å¾ªç¯å†…é‡å¤è®¡ç®—
- âœ… **einsum æ‰¹é‡åŒ–**ï¼šåˆ©ç”¨BLASåº“ä¼˜åŒ–
- âœ… **CUDA kernel**ï¼šç”Ÿäº§ç¯å¢ƒä½¿ç”¨ `selective_scan_cuda.fwd()`

---

## ä¸‰ã€Mamba vs Vision Mamba æ ¸å¿ƒå·®å¼‚

### 3.1 æ¶æ„å¯¹æ¯”è¡¨

| ç»´åº¦ | æ ‡å‡† Mambaï¼ˆè¯­è¨€æ¨¡å‹ï¼‰ | Vision Mambaï¼ˆè§†è§‰æ¨¡å‹ï¼‰ |
|------|----------------------|------------------------|
| **è¾“å…¥ç±»å‹** | æ–‡æœ¬tokenåºåˆ— (1D) | å›¾åƒpatchåºåˆ— (2Dâ†’1D) |
| **è¾“å…¥ç»´åº¦** | `(B, L, d_model)` | `(B, L, d_model)` |
| **å…¸å‹åºåˆ—é•¿åº¦** | 512-4096 tokens | 196-1024 patches |
| **Embedding** | Token Embedding | Patch Embedding (Conv2d) |
| **ä½ç½®ç¼–ç ** | RoPEï¼ˆé€šå¸¸ï¼‰ | ç»å¯¹ä½ç½®/RoPEï¼ˆå¯é€‰ï¼‰ |
| **ç‰¹æ®ŠToken** | [BOS][EOS][PAD] | CLS tokenï¼ˆå¯é€‰ï¼‰ |
| **æ‰«ææ–¹å‘** | å•å‘ï¼ˆå› æœï¼‰ | **åŒå‘ï¼ˆBiMambaï¼‰** |
| **SSMå‚æ•°** | Î”, B, C è¾“å…¥ä¾èµ– | Î”, B, C è¾“å…¥ä¾èµ– |
| **çŠ¶æ€çŸ©é˜µ A** | å›ºå®šï¼ˆå¯å­¦ä¹ ï¼‰ | å›ºå®šï¼ˆå¯å­¦ä¹ ï¼‰ |
| **å·ç§¯å±‚** | Causal Conv1d (d_conv=4) | Causal Conv1d (d_conv=4) |
| **è¾“å‡ºä»»åŠ¡** | ä¸‹ä¸€ä¸ªtokené¢„æµ‹ | å›¾åƒåˆ†ç±»/æ£€æµ‹/åˆ†å‰² |
| **å…¸å‹åº”ç”¨** | æ–‡æœ¬ç”Ÿæˆã€é—®ç­” | å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ |

---

### 3.2 è¯¦ç»†å·®å¼‚åˆ†æ

#### å·®å¼‚1: è¾“å…¥è¡¨ç¤º

**æ ‡å‡† Mamba**:
```python
# æ–‡æœ¬è¾“å…¥
input_ids = [101, 2054, 2003, ...]  # Token IDs
embeddings = embedding_layer(input_ids)  # (B, L, d_model)
```

**Vision Mamba**:
```python
# å›¾åƒè¾“å…¥
image = torch.randn(1, 3, 224, 224)  # (B, C, H, W)
x = patch_embed(image)  # Conv2d â†’ (B, 196, d_model)
```

**æ ¸å¿ƒåŒºåˆ«**ï¼š
- Mamba: ç¦»æ•£token â†’ æŸ¥è¡¨Embedding
- Vim: è¿ç»­åƒç´  â†’ å·ç§¯æŠ•å½±

---

#### å·®å¼‚2: ä½ç½®ç¼–ç 

**æ ‡å‡† Mamba**ï¼ˆé€šå¸¸ä½¿ç”¨RoPEï¼‰:
```python
# æ¯å±‚åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
hidden_states = rope(hidden_states)
```

**Vision Mamba**ï¼ˆç»å¯¹ä½ç½®ç¼–ç ä¸ºä¸»ï¼‰:
```python
# è¾“å…¥å±‚ä¸€æ¬¡æ€§åŠ æ³•
x = x + self.pos_embed  # å­¦ä¹ åˆ°çš„ä½ç½®ç¼–ç 
```

**ä¸ºä»€ä¹ˆä¸åŒï¼Ÿ**
- **æ–‡æœ¬**ï¼šé¡ºåºå¼ºç›¸å…³ â†’ RoPEæ›´é€‚åˆ
- **å›¾åƒ**ï¼š2Dç©ºé—´ç»“æ„ â†’ ç»å¯¹ä½ç½®è¶³å¤Ÿ

---

#### å·®å¼‚3: æ‰«ææœºåˆ¶

**æ ‡å‡† Mambaï¼ˆå•å‘å› æœæ‰«æï¼‰**:
```python
# åªçœ‹è¿‡å»çš„token
for t in range(seq_len):
    h[t] = f(h[t-1], u[t])  # åªä¾èµ– t-1
    y[t] = g(h[t])
```

**Vision Mambaï¼ˆåŒå‘éå› æœæ‰«æï¼‰**:
```python
# å‰å‘æ‰«æ
h_forward = forward_scan(x)

# åå‘æ‰«æï¼ˆç¿»è½¬åºåˆ—ï¼‰
h_backward = forward_scan(x.flip([-1])).flip([-1])

# èåˆ
h = h_forward + h_backward
```

**åŸå› **ï¼š
- **è¯­è¨€å»ºæ¨¡**ï¼šè‡ªå›å½’ï¼Œå¿…é¡»å› æœï¼ˆä¸èƒ½çœ‹æœªæ¥ï¼‰
- **å›¾åƒåˆ†ç±»**ï¼šéè‡ªå›å½’ï¼Œå¯ä»¥çœ‹å…¨å±€

---

#### å·®å¼‚4: CLS Token æœºåˆ¶

**æ ‡å‡† Mamba**:
```python
# é€šå¸¸æ²¡æœ‰ä¸“é—¨çš„CLS token
# ç›´æ¥ä½¿ç”¨æœ€åä¸€ä¸ªhidden state
final_feat = hidden_states[:, -1, :]
```

**Vision Mamba**:
```python
# æ˜¾å¼æ’å…¥CLS token
cls_token = self.cls_token.expand(B, -1, -1)
x = torch.cat((x[:, :mid, :], cls_token, x[:, mid:, :]), dim=1)

# æå–CLS tokenç‰¹å¾
cls_feat = hidden_states[:, mid, :]
```

**è®¾è®¡ç†ç”±**ï¼š
- **Mamba**ï¼šåºåˆ—æœ€åè‡ªç„¶èšåˆå…¨å±€ä¿¡æ¯
- **Vim**ï¼šå€Ÿé‰´ViTï¼Œä¸“é—¨tokenèšåˆå…¨å±€ç‰¹å¾

---

#### å·®å¼‚5: æ¨¡å‹æ·±åº¦å’Œå®½åº¦

**æ ‡å‡† Mamba**ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰:
```python
# å…¸å‹é…ç½®ï¼ˆMamba-370Mï¼‰
d_model = 1024
n_layer = 48
d_state = 16
d_conv = 4
expand = 2
```

**Vision Mamba**ï¼ˆè§†è§‰æ¨¡å‹ï¼‰:
```python
# Vim-Smallé…ç½®
d_model = 384
n_layer = 24
d_state = 16
d_conv = 4
expand = 2
```

**åŒºåˆ«**ï¼š
- Mamba: æ›´æ·±ï¼ˆ48å±‚ vs 24å±‚ï¼‰
- Vim: æ›´çª„ï¼ˆ384ç»´ vs 1024ç»´ï¼‰
- åŸå› : è§†è§‰ä»»åŠ¡å¯¹æ·±åº¦éœ€æ±‚è¾ƒå°

---

### 3.3 ä»£ç å±‚é¢å¯¹æ¯”

#### Mamba Blockï¼ˆæ ‡å‡†ï¼‰

```python
# ä»£ç ä½ç½®: mamba_ssm/modules/mamba_simple.py:186

class Mamba(nn.Module):
    def forward(self, hidden_states):
        xz = self.in_proj(hidden_states)  # è¾“å…¥æŠ•å½±
        x, z = xz.chunk(2, dim=-1)
        
        # å› æœå·ç§¯
        x = causal_conv1d_fn(x, self.conv1d.weight, ...)
        
        # SSMå‚æ•°
        x_dbl = self.x_proj(x)
        dt, B, C = torch.split(x_dbl, [...])
        
        # å•å‘Selective Scan
        y = selective_scan_fn(x, dt, A, B, C, D, z=z, ...)
        
        return self.out_proj(y)
```

#### Vision Mamba Blockï¼ˆåŒå‘ï¼‰

```python
# ä»£ç ä½ç½®: models_mamba.py:233

class Mamba(nn.Module):
    def forward(self, hidden_states):
        xz = self.in_proj(hidden_states)
        
        # å‰å‘åˆ†æ”¯
        out_f = mamba_inner_fn_no_out_proj(
            xz, ..., A, ...
        )
        
        # åå‘åˆ†æ”¯ï¼ˆç‹¬ç«‹å‚æ•°ï¼‰
        out_b = mamba_inner_fn_no_out_proj(
            xz.flip([-1]),  # ç¿»è½¬
            ..., A_b, ...
        )
        
        # åˆå¹¶
        out = out_f + out_b.flip([-1])
        return self.out_proj(out)
```

---

## å››ã€æ€§èƒ½ç‰¹æ€§å¯¹æ¯”

### 4.1 è®¡ç®—å¤æ‚åº¦

**Time Complexity**:

| æ“ä½œ | Mamba | Vision Mamba | æ³¨é‡Š |
|------|-------|--------------|------|
| Self-Attention | - | O(LÂ²Â·D) | Transformer baseline |
| Selective Scan | **O(LÂ·DÂ·N)** | **2Ã—O(LÂ·DÂ·N)** | N<<D, åŒå‘2å€ |
| Conv1d | O(LÂ·DÂ·k) | 2Ã—O(LÂ·DÂ·k) | k=4 (kernel size) |
| Linear Proj | O(LÂ·DÂ²) | O(LÂ·DÂ²) | ç›¸åŒ |
| **Total** | **O(LÂ·DÂ·N)** | **O(LÂ·DÂ·N)** | N=16, çº¿æ€§å¤æ‚åº¦ |

**Space Complexity**:

| ç»„ä»¶ | Mamba | Vision Mamba |
|------|-------|--------------|
| Hidden States | O(BÂ·LÂ·D) | O(BÂ·LÂ·D) |
| SSM States (conv) | O(BÂ·DÂ·k) | 2Ã—O(BÂ·DÂ·k) |
| SSM States (scan) | O(BÂ·DÂ·N) | 2Ã—O(BÂ·DÂ·N) |
| **Total** | **O(BÂ·LÂ·D)** | **O(BÂ·LÂ·D)** |

---

### 4.2 å®é™…æ€§èƒ½æ•°æ®

**æ¨ç†é€Ÿåº¦**ï¼ˆImageNet, Batch Inferenceï¼‰:

| æ¨¡å‹ | åˆ†è¾¨ç‡ | ååé‡ | ç›¸å¯¹DeiT |
|------|--------|--------|----------|
| DeiT-S | 224Ã—224 | 100 img/s | 1.0x |
| Vim-S | 224Ã—224 | **180 img/s** | 1.8x |
| DeiT-S | 1248Ã—1248 | 5 img/s | 1.0x |
| Vim-S | 1248Ã—1248 | **14 img/s** | **2.8x** |

**GPUå†…å­˜å ç”¨**ï¼ˆ1248Ã—1248åˆ†è¾¨ç‡ï¼‰:

| æ¨¡å‹ | æ˜¾å­˜ | ç›¸å¯¹DeiT |
|------|------|----------|
| DeiT-S | 15.2 GB | 1.0x |
| Vim-S | **2.0 GB** | **0.13xï¼ˆèŠ‚çœ86.8%ï¼‰** |

**å…³é”®å‘ç°**ï¼š
- âœ… **é«˜åˆ†è¾¨ç‡ä¼˜åŠ¿æ˜æ˜¾**ï¼šåˆ†è¾¨ç‡è¶Šé«˜ï¼ŒVimä¼˜åŠ¿è¶Šå¤§
- âœ… **å†…å­˜æ•ˆç‡æé«˜**ï¼šO(L) vs O(LÂ²) çš„æœ¬è´¨ä¼˜åŠ¿
- âœ… **é€‚åˆå¯†é›†é¢„æµ‹ä»»åŠ¡**ï¼šæ£€æµ‹ã€åˆ†å‰²ç­‰é«˜åˆ†è¾¨ç‡ä»»åŠ¡

---

## äº”ã€Selective Scan çš„"é€‰æ‹©æ€§"æœ¬è´¨

### 5.1 ä»€ä¹ˆæ˜¯"é€‰æ‹©æ€§"ï¼Ÿ

**ä¼ ç»ŸSSM (S4)**:
```python
A, B, C, D = å›ºå®šå‚æ•°  # æ‰€æœ‰è¾“å…¥å…±äº«å‚æ•°
```

**Mamba (Selective SSM)**:
```python
Î” = f(x)  # æ—¶é—´æ­¥é•¿éšè¾“å…¥å˜åŒ–
B = g(x)  # è¾“å…¥çŸ©é˜µéšè¾“å…¥å˜åŒ–
C = h(x)  # è¾“å‡ºçŸ©é˜µéšè¾“å…¥å˜åŒ–
```

### 5.2 åœ¨ Vision Mamba ä¸­çš„ä½“ç°

**ç¤ºä¾‹ï¼šå¤„ç†ä¸åŒç±»å‹çš„patch**

```python
# å‰æ™¯patchï¼ˆæœ‰ä¿¡æ¯ï¼‰
Î”_foreground = large  â‡’  Ä€ = exp(Î”Â·A) â‰ˆ è¾ƒå¤§å€¼
â‡’ çŠ¶æ€ä¿æŒæ›´å¤šï¼Œè®°å¿†æ›´é•¿

# èƒŒæ™¯patchï¼ˆæ— ä¿¡æ¯ï¼‰
Î”_background = small  â‡’  Ä€ = exp(Î”Â·A) â‰ˆ è¾ƒå°å€¼
â‡’ çŠ¶æ€å¿«é€Ÿé—å¿˜ï¼Œè·³è¿‡æ— ç”¨ä¿¡æ¯
```

**è¿™å°±æ˜¯"é€‰æ‹©æ€§"**ï¼š
- **é‡è¦ä¿¡æ¯** â†’ å¤§Î” â†’ å¼ºè®°å¿†
- **æ— ç”¨ä¿¡æ¯** â†’ å°Î” â†’ å¿«é—å¿˜

---

## å…­ã€Vision Mamba çš„ç‹¬ç‰¹ä¼˜åŠ¿

### 6.1 ç›¸æ¯”CNN

| ç‰¹æ€§ | CNN | Vision Mamba |
|------|-----|--------------|
| æ„Ÿå—é‡ | å±€éƒ¨ï¼ˆå—é™äºå·ç§¯æ ¸ï¼‰ | **å…¨å±€ï¼ˆSSMé€’æ¨ï¼‰** |
| é•¿è·ç¦»ä¾èµ– | éœ€è¦æ·±å±‚å †å  | **ä¸€å±‚å³å¯** |
| è®¡ç®—å¤æ‚åº¦ | O(LÂ·KÂ²) | O(LÂ·DÂ·N) |
| å‚æ•°å…±äº« | ç©ºé—´å…±äº« | æ—¶é—´+ç©ºé—´å…±äº« |

### 6.2 ç›¸æ¯”Transformer

| ç‰¹æ€§ | Transformer (ViT) | Vision Mamba |
|------|-------------------|--------------|
| è®¡ç®—å¤æ‚åº¦ | **O(LÂ²Â·D)** | **O(LÂ·DÂ·N)** |
| å†…å­˜å ç”¨ | O(LÂ²) | **O(L)** |
| é«˜åˆ†è¾¨ç‡å‹å¥½æ€§ | **ä½** | **é«˜** |
| å…¨å±€å»ºæ¨¡ | Self-Attention | **Selective Scan** |
| å½’çº³åç½® | å¼± | ä¸­ç­‰ï¼ˆåºåˆ—ç»“æ„ï¼‰ |

### 6.3 Vision Mamba æœ€é€‚åˆçš„åœºæ™¯

1. **é«˜åˆ†è¾¨ç‡å›¾åƒ**ï¼š
   - åŒ»å­¦å½±åƒ (1024Ã—1024+)
   - å«æ˜Ÿå›¾åƒ (2048Ã—2048+)
   - è§†é¢‘å¸§ (1920Ã—1080)

2. **å¯†é›†é¢„æµ‹ä»»åŠ¡**ï¼š
   - è¯­ä¹‰åˆ†å‰²
   - ç›®æ ‡æ£€æµ‹
   - å®ä¾‹åˆ†å‰²

3. **èµ„æºå—é™ç¯å¢ƒ**ï¼š
   - ç§»åŠ¨è®¾å¤‡
   - è¾¹ç¼˜è®¡ç®—
   - å®æ—¶åº”ç”¨

---

## ä¸ƒã€Vision Mamba çš„å±€é™æ€§

### 7.1 ç†è®ºå±€é™

1. **åºåˆ—åŒ–æŸå¤±**ï¼š
   ```python
   # 2Då›¾åƒ â†’ 1Dåºåˆ—
   # ä¸¢å¤±äº†2Dé‚»åŸŸå…³ç³»
   image (H, W) â†’ patches (H/P Ã— W/P)
   ```
   - è™½ç„¶åŒå‘æ‰«æç¼“è§£ï¼Œä½†ä¸å¦‚2Då·ç§¯ç›´æ¥

2. **ä½ç½®ä¾èµ–æ€§**ï¼š
   - éœ€è¦é¢å¤–çš„ä½ç½®ç¼–ç 
   - ä¸å¦‚CNNè‡ªå¸¦ç©ºé—´å½’çº³åç½®

3. **åºåˆ—é¡ºåºæ•æ„Ÿ**ï¼š
   - Patché¡ºåºå½±å“ç»“æœ
   - ä¸å¦‚Transformerçš„ç½®æ¢ä¸å˜æ€§

### 7.2 å®è·µå±€é™

1. **è®­ç»ƒç¨³å®šæ€§**ï¼š
   - SSMå‚æ•°åˆå§‹åŒ–æ•æ„Ÿ
   - éœ€è¦careful tuning

2. **è¿ç§»å­¦ä¹ **ï¼š
   - é¢„è®­ç»ƒæ¨¡å‹è¾ƒå°‘
   - ç”Ÿæ€ç³»ç»Ÿä¸å¦‚ViTæˆç†Ÿ

3. **ç¡¬ä»¶ä¼˜åŒ–**ï¼š
   - éœ€è¦é«˜æ•ˆçš„Selective Scanå®ç°
   - CPUä¸Šæ€§èƒ½æå‡æœ‰é™ï¼ˆæœ¬é¡¹ç›®å…³æ³¨ç‚¹ï¼‰

---

## å…«ã€CPUä¼˜åŒ–æ–¹å‘ï¼ˆåŸºäºæœ¬é¡¹ç›®ï¼‰

### 8.1 å½“å‰å®ç°

**VisionMambaä½¿ç”¨çš„Selective Scan**:
```python
# ä»£ç ä½ç½®: selective_scan_interface.py:132

def selective_scan_ref(u, delta, A, B, C, ...):
    # çº¯PyTorchå®ç°ï¼ˆCPUå‹å¥½ï¼‰
    for i in range(seq_len):
        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
        y = torch.einsum('bdn,bn->bd', x, C[:, :, i])
    return y
```

### 8.2 ä¼˜åŒ–ç­–ç•¥ï¼ˆç»§æ‰¿è‡ª Mamba_CPU/ï¼‰

#### ç­–ç•¥1: ä¸¤é˜¶æ®µè®¡ç®—ï¼ˆå·²éªŒè¯ï¼Œ2.18xåŠ é€Ÿï¼‰

```python
# åŸå§‹æ–¹å¼: é€æ­¥è®¡ç®—
for i in range(L):
    x[i] = A[i] * x[i-1] + B[i]
    y[i] = C[i] @ x[i]  # æ¯æ¬¡einsum

# ä¼˜åŒ–æ–¹å¼: åˆ†ç¦»è®¡ç®—
# é˜¶æ®µ1: æ‰¹é‡è®¡ç®—æ‰€æœ‰x
for i in range(1, L):
    x[i] = A[i] * x[i-1] + x[i]  # åŸåœ°æ›´æ–°

# é˜¶æ®µ2: ä¸€æ¬¡å¤§einsum
y = torch.einsum('bldn,bln->bld', x, C)  # æ‰¹é‡è®¡ç®—
```

#### ç­–ç•¥2: C++ + PyTorch APIï¼ˆå·²éªŒè¯ï¼Œ2.78xåŠ é€Ÿï¼‰

```cpp
// ä½¿ç”¨torch::çš„ä¼˜åŒ–å‡½æ•°
auto deltaA = torch::exp(delta.unsqueeze(-1) * A);  // MKLä¼˜åŒ–
auto deltaB_u = delta.unsqueeze(-1) * B * u.unsqueeze(-1);

// é€’æ¨éƒ¨åˆ†ä»éœ€forå¾ªç¯ï¼ˆä¸²è¡Œä¾èµ–ï¼‰
for (int64_t i = 1; i < seq_len; i++) {
    x[i] = deltaA[i] * x[i-1] + deltaB_u[i];
}

// è¾“å‡ºæ‰¹é‡è®¡ç®—
auto y = torch::sum(x.unsqueeze(-2) * C.unsqueeze(-3), /*dim=*/-1);
```

#### ç­–ç•¥3: åŒå‘å¹¶è¡ŒåŒ–

```cpp
// å‰å‘å’Œåå‘å¯ä»¥å¹¶è¡Œ
#pragma omp parallel sections
{
    #pragma omp section
    {
        // å‰å‘æ‰«æ
        forward_scan(x, deltaA_f, deltaB_u_f);
    }
    
    #pragma omp section
    {
        // åå‘æ‰«æ
        backward_scan(x_b, deltaA_b, deltaB_u_b);
    }
}

// åˆå¹¶ç»“æœ
out = out_f + out_b_flipped;
```

### 8.3 é¢„æœŸæ€§èƒ½æå‡

åŸºäº Mamba_CPU çš„å®éªŒç»“æœï¼š

| ä¼˜åŒ–å±‚çº§ | æ–¹æ³• | é¢„æœŸåŠ é€Ÿæ¯” | å®ç°éš¾åº¦ |
|---------|------|-----------|---------|
| Level 1 | ä¸¤é˜¶æ®µè®¡ç®—ï¼ˆPythonï¼‰ | 2.2x | ğŸŸ¢ ä½ |
| Level 2 | C++ + torch:: API | 2.8x | ğŸŸ¡ ä¸­ |
| Level 3 | åŒå‘å¹¶è¡Œ | 3.5x | ğŸŸ¡ ä¸­ |
| Level 4 | æ•°æ®é‡æ’ï¼ˆcacheå‹å¥½ï¼‰ | 4.0x | ğŸ”´ é«˜ |

**Vision Mamba ç‰¹æœ‰æŒ‘æˆ˜**ï¼š
- Patchåºåˆ—æ›´é•¿ï¼ˆ196 vs 128ï¼‰â†’ ä¼˜åŒ–æ”¶ç›Šæ›´å¤§
- åŒå‘æ‰«æ â†’ å¹¶è¡ŒåŒ–æ½œåŠ›
- Batchæ¨ç† â†’ å¤–å±‚batchå¹¶è¡Œ

---

## ä¹ã€æ€»ç»“

### 9.1 æ ¸å¿ƒè¦ç‚¹

1. **Vision Mamba = Mamba SSM + Vision-Specific Adaptations**
   - æ ¸å¿ƒSSMæœºåˆ¶ä¸å˜
   - æ·»åŠ Patch Embeddingã€åŒå‘æ‰«æã€CLS token

2. **å…³é”®åˆ›æ–°**ï¼š
   - âœ… **åŒå‘BiMamba**ï¼šå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡
   - âœ… **çº¿æ€§å¤æ‚åº¦**ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒé«˜æ•ˆå¤„ç†
   - âœ… **é€‰æ‹©æ€§æœºåˆ¶**ï¼šåŠ¨æ€è°ƒæ•´ä¿¡æ¯æµ

3. **é€‚ç”¨åœºæ™¯**ï¼š
   - âœ… é«˜åˆ†è¾¨ç‡å›¾åƒ
   - âœ… å¯†é›†é¢„æµ‹ä»»åŠ¡
   - âœ… å†…å­˜å—é™ç¯å¢ƒ

4. **ä¸è¶³**ï¼š
   - âš  2Dç©ºé—´ç»“æ„å»ºæ¨¡å¼±äºCNN
   - âš  ç”Ÿæ€ç³»ç»Ÿä¸å¦‚ViTæˆç†Ÿ
   - âš  CPUä¼˜åŒ–ç©ºé—´æœ‰é™ï¼ˆé€’æ¨ä¾èµ–ï¼‰

### 9.2 Mamba vs Vision Mamba ä¸€å¥è¯æ€»ç»“

| æ¨¡å‹ | æ ¸å¿ƒä»»åŠ¡ | å…³é”®ç‰¹å¾ |
|------|---------|---------|
| **Mamba** | è¯­è¨€å»ºæ¨¡ | å•å‘å› æœæ‰«æï¼Œé•¿åºåˆ—å»ºæ¨¡ |
| **Vision Mamba** | è§†è§‰ç†è§£ | **åŒå‘éå› æœæ‰«æ**ï¼Œé«˜åˆ†è¾¨ç‡é«˜æ•ˆ |

**å…±åŒç‚¹**ï¼šéƒ½åŸºäº Selective State Space Model
**æœ¬è´¨åŒºåˆ«**ï¼šå› æœæ€§ï¼ˆå•å‘ vs åŒå‘ï¼‰

---

## åã€å‚è€ƒèµ„æ–™

### è®ºæ–‡
1. **Mamba**: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
2. **Vision Mamba**: [Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](https://arxiv.org/abs/2401.09417)

### ä»£ç 
- Vision Mambaå®˜æ–¹å®ç°: https://github.com/hustvl/Vim
- Mambaå®˜æ–¹å®ç°: https://github.com/state-spaces/mamba

### æœ¬é¡¹ç›®ç›¸å…³æ–‡æ¡£
- [`mamba_math_derivation.md`](mamba_math_derivation.md) - Mambaæ•°å­¦æ¨å¯¼
- [`design.md`](design.md) - CPUä¼˜åŒ–è®¾è®¡
- [`Mamba_CPU/selective_scan.cpp`](Mamba_CPU/selective_scan.cpp) - C++ä¼˜åŒ–å®ç°

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**ä½œè€…**: KeJi  
**æ—¥æœŸ**: 2025-12-22
